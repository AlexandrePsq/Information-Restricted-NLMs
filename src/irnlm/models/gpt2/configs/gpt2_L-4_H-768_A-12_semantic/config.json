{'activation_function': 'gelu_new', 'attn_pdrop': 0.1, 'bos_token_id': 1, 'embd_pdrop': 0.1, 'eos_token_id': 2, 'gradient_checkpointing': false, 'initializer_range': 0.02, 'layer_norm_epsilon': 1e-05, 'model_type': 'gpt2', 'n_ctx': 512, 'n_embd': 768, 'n_head': 12, 'n_inner': null, 'n_layer': 4, 'n_positions': 512, 'resid_pdrop': 0.1, 'scale_attn_weights': true, 'summary_activation': null, 'summary_first_dropout': 0.1, 'summary_proj_to_labels': true, 'summary_type': 'cls_index', 'summary_use_proj': true, 'transformers_version': '4.9.1', 'use_cache': true, 'vocab_size': 50000, 'output_hidden_states': true, 'output_attentions': false}
