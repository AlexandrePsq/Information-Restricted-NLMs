#####
# From Appendix A.3 of the BERT paper, when fine-tuning BERT on a specific task, 
# the authors recommend:
#     - Batch size: 16, 32
#     - Learning rate (Adam): 5e-5, 3e-5, 2e-5
#     - Number of epochs: 2, 3, 4
#     - eps = 1e-8
#####


# General
task: language-modeling # Task on which to fine-tune the model ['POS-tagging', 'NER', 'sentiment-analysis', 'sentence-classification', 'mask-language-modeling]
seed: 1111 # Seed for reproductibility
output_dir: /neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/derivatives/fMRI/models/english/GPT-2/GPT-2_L-12_H-768_default_tokenizer
log_file: logs_training.txt
local_rank: -1
do_test: True
do_train: True
do_validation: True
metric_name: classification

# Datasets
dataset_name: 'gpt2_'
extra: 'default-tokenizer_'
dataset_dir: "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/english/all_training" # Path/URL to the folder containing the dataset to use for fine-tuning
use_output_mask: False # for token classification
nb_splits: 5

# Data Loader
train_size_percentage: 0.9
batch_size: 32
batch_size_eval: 64

# Model & Tokenizer
pretrained_model: gpt2 # Name of (or path to) the pre-trained BERT model to use
pretrained_tokenizer: gpt2 # Name of (or path to) the pre-trained BERT tokenizer to use
config_path: "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/models/english/gpt2_L-12_H-768_A-12/config.json"
output_attentions: True
output_hidden_states: True
max_length: 512
masking_proportion: 15
start_from_scratch: True
strip_accents: True
tokenizer_from_scratch: False
lowercase: True
vocab_size: 50001
min_frequency: 2
limit_alphabet: 1000
context_size: 507
device_map:
    0:
        - 0
        - 1
        - 2
        - 3
        - 4
        - 5
    1:
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11

# Optimizer
learning_rate: 1e-4 # Default is 5e-5
adam_epsilon: 1e-8 # Adam_epsilon  - default is 1e-8.

# Scheduler
num_warmup_steps: 0 # Default value in run_glue.py

# Training
nb_epochs: 5 # BERT authors recommended between 2 and 4
nb_checkpoints: 30 # number of checkpoints at which to save model state
start_epoch: 1
init_checkpoints: 1
