#####
# From Appendix A.3 of the BERT paper, when fine-tuning BERT on a specific task, 
# the authors recommend:
#     - Batch size: 16, 32
#     - Learning rate (Adam): 5e-5, 3e-5, 2e-5
#     - Number of epochs: 2, 3, 4
#     - eps = 1e-8
#####


# General
task: language-modeling # Task on which to fine-tune the model ['POS-tagging', 'NER', 'sentiment-analysis', 'sentence-classification', 'mask-language-modeling]
seed: 1111 # Seed for reproductibility
output_dir: /neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/derivatives/fMRI/models/english/GPT-2/GPT-2_L-4_H-768_corrected
log_file: logs_evaluation.txt
local_rank: -1
todo: 'dev'
metric_name: classification
output_name: 'start-epoch-0'
model_path: /neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/derivatives/fMRI/models/english/GPT-2/GPT-2_L-4_H-768_corrected/start-epoch-0

# Datasets
dataset_name: 'gpt2_'
extra: 'corrected_'
dataset_dir: "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/english/all_training" # Path/URL to the folder containing the dataset to use for fine-tuning
use_output_mask: False # for token classification
nb_splits: 5

# Data Loader
train_size_percentage: 0.9
batch_size_eval: 64

# Model & Tokenizer
pretrained_model: gpt2 # Name of (or path to) the pre-trained BERT model to use
pretrained_tokenizer: gpt2 # Name of (or path to) the pre-trained BERT tokenizer to use
config_path: "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/models/english/gpt2_L-4_H-768_Gutenberg/config.json"
output_attentions: True
output_hidden_states: True
max_length: 512
masking_proportion: 15
start_from_scratch: True
strip_accents: True
tokenizer_from_scratch: True
lowercase: True
vocab_size: 50001
min_frequency: 2
limit_alphabet: 1000
context_size:

