#####
# From Appendix A.3 of the BERT paper, when fine-tuning BERT on a specific task,
# the authors recommend:
#     - Batch size: 16, 32
#     - Learning rate (Adam): 5e-5, 3e-5, 2e-5
#     - Number of epochs: 2, 3, 4
#     - eps = 1e-8
#####

# General
task: language-modeling # Task on which to fine-tune the model ['POS-tagging', 'NER', 'sentiment-analysis', 'sentence-classification', 'mask-language-modeling]
seed: 1111 # Seed for reproductibility
output_dir: /neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/derivatives/fMRI/models/french/irnlm/GPT-2/GPT-2_L-4_H-768_syntactic
model: end-epoch-2
log_file: logs_eval.log
local_rank: -1
do_test: True
do_train: False
do_validation: True
metric_name: classification

# Datasets
dataset_dir: "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data" #english/all_training" # Path/URL to the folder containing the dataset to use for fine-tuning
use_output_mask: False # for token classification
nb_splits: 1 #5 for french

# Data Loader
train_size_percentage: 0.9
batch_size: 32
batch_size_eval: 64

# Model & Tokenizer
model_type: syntactic
pretrained_model: # gpt2 # Name of (or path to) the pre-trained BERT model to use
pretrained_tokenizer: # gpt2 # Name of (or path to) the pre-trained BERT tokenizer to use
config_path: "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/Information-Restrited-NLMs/src/irnlm/models/gpt2/configs/gpt2_L-4_H-768_A-12_syntactic/config.json"
tokenizer_training_data_paths:
  - "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/training_data.000"
  - "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/training_data.001"
  - "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/training_data.002"
  - "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/training_data.003"
  - "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/training_data.004"
  - "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/training_data.005"
  - "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/training_data.006"
  - "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/training_data.007"
  - "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/training_data.008"
  - "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/training_data.009"
tokenizer_special_tokens:
  - "<pad>" # "[PAD]"
  - "<s>" # "[CLS]"
  - "</s>" # "[SEP]"
  - "<unk>" # "[UNK]"
  - "<mask>" # "[MASK]"
tokenizer_path: "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/syntactic"
train_paths:
  - /neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/CC100_syntactic_train_split-0.pkl
  - /neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/CC100_syntactic_train_split-1.pkl
  - /neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/CC100_syntactic_train_split-2.pkl
  - /neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/CC100_syntactic_train_split-3.pkl
  - /neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/CC100_syntactic_train_split-4.pkl
  - /neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/CC100_syntactic_train_split-5.pkl
  - /neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/CC100_syntactic_train_split-6.pkl
  - /neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/CC100_syntactic_train_split-7.pkl
  - /neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/CC100_syntactic_train_split-8.pkl
  - /neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/CC100_syntactic_train_split-9.pkl
dev_paths:
  #- /neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/CC100_syntactic_dev_split-0.pkl #training_data.010
  - /neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/CC100_syntactic_train_split-0.pkl #training_data.010
test_paths:
  #- /neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/CC100_syntactic_test_split-0.pkl #training_data.011
  - /neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/french/training_data/CC100_syntactic_train_split-0.pkl #training_data.011
output_attentions: False
output_hidden_states: False
max_length: 512
masking_proportion: 15
start_from_scratch: True
strip_accents: True
tokenizer_from_scratch: True
lowercase: True
vocab_size: 1195
min_frequency: 2
limit_alphabet: 1000
context_size: 508
device_map:
